{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate  # 모델 학습 속도 향상\n",
    "# !pip install peft        # LoRA를 포함한 효율적 미세 조정\n",
    "# !pip install bitsandbytes # 양자화 기술을 통한 메모리 최적화\n",
    "# !pip install transformers # 트랜스포머 기반 모델 라이브러리\n",
    "# !pip install datasets     # 데이터셋 로드 및 처리\n",
    "# !pip install trl          # 트랜스포머 기반 강화 학습\n",
    "# !pip install pandas       # 데이터 처리 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jihun.kim/src/cosera-generative-ai/.venv/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login --token ${HF_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cuda:0', 'cuda:1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = []\n",
    "if sys.platform == 'darwin':\n",
    "    device.append[(\"mps\" if torch.backends.mps.is_available() else \"cpu\")]\n",
    "else:\n",
    "    if torch.cuda.is_available() :\n",
    "        for ix in range(torch.cuda.device_count()):\n",
    "            device.append(f\"\"\"cuda:{ix}\"\"\")\n",
    "    else:\n",
    "        device.append(\"cpu\")\n",
    "       \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'url'],\n",
       "    num_rows: 1058\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds =  load_dataset(\"beomi/KoAlpaca-v1.1a\", split=\"train[:5%]\")\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?',\n",
       " 'output': '양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \\n\\n식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\\n\\n 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \\n\\n고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.',\n",
       " 'url': 'https://kin.naver.com/qna/detail.naver?d1id=11&dirId=1116&docId=55320268'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.with_format(\"torch\", device=device[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(row):\n",
    "    return {\n",
    "        'text': f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    " \n",
    "        You are a helpful assistant<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\n",
    " \n",
    "        {row['instruction']}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\n",
    " \n",
    "        {row['output']}<|eot_id|>\"\"\"\n",
    "    }\n",
    " \n",
    "# 판다스 데이터프레임을 데이터셋으로 변환하고, 포맷팅 함수 적용\n",
    "dataset = ds.map(format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model = \"beomi/Llama-3-KoEn-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=device[0],\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n",
    ")\n",
    " \n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "              base_model,\n",
    "              trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 시퀀스 패딩에 eos 토큰 사용\n",
    "tokenizer.padding_side = \"right\"           # 패딩을 오른쪽에 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정\n",
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,       # LoRA의 스케일링 계수 설정\n",
    "    lora_dropout=0.1,    # 드롭아웃을 통해 과적합 방지\n",
    "    r=8,                 # LoRA 어댑터 행렬의 Rank 설정\n",
    "    bias=\"none\",         # 편향 사용 여부 설정\n",
    "    task_type=\"CAUSAL_LM\", # 작업 유형 설정 (Causal LM)\n",
    "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj'] # 적용 모듈 설정\n",
    ")\n",
    " \n",
    "# 모델을 8bit 학습을 위한 상태로 준비. 메모리를 절약하면서도 모델의 성능을 유지할 수 있음\n",
    "model = prepare_model_for_kbit_training(model, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT 어댑터 설정을 모델에 적용\n",
    "model = get_peft_model(model, peft_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 6815744\n",
      "all model parameters: 8037076992\n",
      "percentage of trainable model parameters: 0.08%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device[0])\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "# training_params = TrainingArguments(\n",
    "#     output_dir=\"./results\",              # 결과 저장 경로\n",
    "#     num_train_epochs=10,                 # 학습 에폭 수\n",
    "#     per_device_train_batch_size=1,       # 배치 사이즈\n",
    "#     learning_rate=2e-4,                  # 학습률 설정\n",
    "#     save_steps=1000,                     # 저장 빈도\n",
    "#     logging_steps=50,                    # 로그 출력 빈도\n",
    "#     fp16=True                            # 16-bit 부동 소수점 사용 (메모리 절약),\n",
    "# )\n",
    "\n",
    "training_params = SFTConfig(\n",
    "    output_dir=\"./results\",              # 결과 저장 경로\n",
    "    num_train_epochs=10,                 # 학습 에폭 수\n",
    "    per_device_train_batch_size=4,       # 배치 사이즈\n",
    "    learning_rate=2e-4,                  # 학습률 설정\n",
    "    save_steps=1000,                     # 저장 빈도\n",
    "    logging_steps=50,                    # 로그 출력 빈도\n",
    "    fp16=True,                            # 16-bit 부동 소수점 사용 (메모리 절약),\n",
    "    max_seq_length=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1469679/1806763960.py:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Tokenizing train dataset: 100%|██████████| 1058/1058 [00:01<00:00, 914.98 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1058/1058 [00:00<00:00, 1614.20 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# SFTTrainer를 사용해 학습 실행\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/jihun.kim/src/cosera-generative-ai/.venv/lib64/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jihun.kim/src/cosera-generative-ai/.venv/lib64/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1330' max='1330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1330/1330 2:27:52, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.891800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.675300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.624900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.623400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.585700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.473300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.465500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.419800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.140900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.067100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.948000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jihun.kim/src/cosera-generative-ai/.venv/lib64/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jihun.kim/src/cosera-generative-ai/.venv/lib64/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1330, training_loss=1.317376254375716, metrics={'train_runtime': 8878.2355, 'train_samples_per_second': 1.192, 'train_steps_per_second': 0.15, 'total_flos': 2.121489164916818e+17, 'train_loss': 1.317376254375716})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=256)\n",
    " \n",
    "def generate_and_stop(prompt):\n",
    "    result = pipe(f\"{prompt}\")[0]['generated_text']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n \\nYou are a helpful assistant<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n \\n양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\nYou are a helpful assistant\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    " \n",
    "You are a helpful assistant<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\n",
    " \n",
    "양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    " \n",
    "generate_and_stop(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./results/peft-tuned-Llama-3-KoEn-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:10<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "peft_tuned_model = \"./results/peft-tuned-Llama-3-KoEn-8B\"\n",
    "tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_tuned_model,\n",
    "    device_map=device[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_tokenizer = AutoTokenizer.from_pretrained(\n",
    "              peft_tuned_model,\n",
    "              trust_remote_code=True,\n",
    "              device=device[1])\n",
    "tuned_tokenizer.pad_token = tokenizer.eos_token  # 시퀀스 패딩에 eos 토큰 사용\n",
    "tuned_tokenizer.padding_side = \"right\"           # 패딩을 오른쪽에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    }
   ],
   "source": [
    "tuned_pipe = pipeline(task=\"text-generation\", model=tuned_model, tokenizer=tuned_tokenizer, max_length=256)\n",
    " \n",
    "def generate_and_stop_with_tuned(prompt):\n",
    "    result = tuned_pipe(f\"{prompt}\")[0]['generated_text']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n \\nYou are a helpful assistant<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n \\n양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n양파는 식물의 꽃줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. 고구마는 줄기에서 뿌리를 내리며, 이 뿌리가 열매를 맺는 것입니다. 반면, 양파는 꽃줄기에서 열리는 열매입니다. 양파는 줄기가 없고, 잎과 꽃줄기가 바로 연결되어 있습니다. 이러한 차이점을 잘 기억해두시면 좋을 것 같습니다.�\\n�assistant�\\n \\n고구마는 뿌리인가요? \\n고구마는 줄기에서 뿌리를 내리며, 이 뿌리가 열매를 맺습니다. 줄기에서 뻗어나온 고구마는 땅속에서 자라며, 이때 자라는 것이 뿌리입니다. 따라서 고구마는 식물의 뿌리 부분입니다. 양파는 꽃줄기에서 열리는 열매입니다. 이러한'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_stop_with_tuned(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
